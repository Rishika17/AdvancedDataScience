{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Quiz bowl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-PEcB5FB5KH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "547f435f-8aae-424c-d373-c25180326335"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install --upgrade category_encoders\n",
        "import category_encoders as ce \n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "import re\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/a1/f7a22f144f33be78afeb06bfa78478e8284a64263a3c09b1ef54e673841e/category_encoders-2.0.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 28.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkvjTY5B5Kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ac40ef0-6352-4d5e-ab10-21deebbd86ca"
      },
      "source": [
        "\"\"\"\n",
        "define a function for auto ml\n",
        "define a function to prepare data\n",
        "define a function to clean text (also define stopwords, lemmatizer, punctuations)\n",
        "\"\"\"\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def auto_ML(model, features, Xtrain, Ytrain, Xtest, Ytest, folds= 10):\n",
        "    model.fit(Xtrain[features], Ytrain)\n",
        "    prediction= model.predict(Xtest[features])\n",
        "    \n",
        "    # Cross validation\n",
        "    scores= cross_val_score(model, Xtrain[features], Ytrain, cv= folds)\n",
        "    val_score= sum(scores/folds)\n",
        "    \n",
        "    print('Test prediction:\\n{}'.format(prediction))\n",
        "    print('-------------------------------------')\n",
        "    print('Accuracy score: {}'.format(accuracy_score(Ytest, prediction)))\n",
        "    print('-------------------------------------')\n",
        "    print('confusion matrix:\\n {}'.format(confusion_matrix(Ytest, prediction)))\n",
        "    print('-------------------------------------')\n",
        "    print('Cross Validation score: {}'.format(val_score))\n",
        "    \n",
        "    correct_df= pd.DataFrame(columns= Xtrain.columns)\n",
        "    incorrect_df= pd.DataFrame(columns= Xtrain.columns)\n",
        "    \n",
        "    for i, pred in enumerate(prediction):\n",
        "        if pred== Ytest[i]:\n",
        "            correct_df= correct_df.append(Xtest.loc[i, Xtest.columns])\n",
        "        else:\n",
        "            incorrect_df= incorrect_df.append(Xtest.loc[i, Xtest.columns])\n",
        "            \n",
        "    return correct_df, incorrect_df\n",
        "    \n",
        "    \n",
        "def prepare_data(data, split_ratio):\n",
        "    split= int(data.shape[0]*split_ratio)\n",
        "    \n",
        "    X_train= data.loc[:split, :]\n",
        "    X_test= data.loc[split: , :]\n",
        "    \n",
        "    print('train shape: {}'.format(X_train.shape))\n",
        "    print('test shape: {}'.format(X_test.shape))\n",
        "    \n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "\n",
        "stop= stopwords.words('english')\n",
        "exclude = string.punctuation \n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def clean(doc):\n",
        "    stop_free= ' '.join(word for word in doc.lower().split() if word not in stop)\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OpGXXqtNB5Ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "e0e8bdc5-e1a6-498b-b931-ddf1a86232fa"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyaKKrr4B5MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train =pd.read_csv('gdrive/My Drive/train.csv')\n",
        "test = pd.read_csv('gdrive/My Drive/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiWBMf8nB5MW",
        "colab_type": "code",
        "outputId": "101b1a9f-3bbb-4a06-c244-e4ff604f90b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('training data:')\n",
        "train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8160, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oWHq1SYB5Md",
        "colab_type": "code",
        "outputId": "db96a065-b3ae-4e9a-be06-736f7094d4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('testing data: ')\n",
        "test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing data: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(306, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcusxs8oB5Mj",
        "colab_type": "text"
      },
      "source": [
        "### add new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yJqRYYsSB5Ml",
        "colab_type": "code",
        "outputId": "40f608df-3ba8-452e-fc8d-a2f1b8389678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\"\"\"\n",
        "Add new feature: Question length\n",
        "\n",
        "Add new features: Extract max IR score and correscponding page from IR_Wiki Scores\n",
        "Also calculate difference between highest and second highest IR score\n",
        "\n",
        "Add target feature paren_match (true if wiki page and answer match)\n",
        "\n",
        "We will add the following features after splitting into training and testing.\n",
        "1. Apply Weight of Evidence(WOE) encoding to category variable\n",
        "2. Topic modeling\n",
        "\"\"\"\n",
        "\n",
        "#Question length (training data)\n",
        "lens= []\n",
        "for i in range(0, train.shape[0]):\n",
        "    lens.append(len(train.loc[i]['Question Text']))\n",
        "lens\n",
        "train['Quest len']= lens\n",
        "print('Question length created for training data')\n",
        "\n",
        "#Question length (testing data)\n",
        "lens= []\n",
        "for i in range(0, test.shape[0]):\n",
        "    lens.append(len(test.loc[i]['Question Text']))\n",
        "lens\n",
        "test['Quest len']= lens\n",
        "print('Question length created for testing data')\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "\n",
        "# Max IR score and corresponding page (training data)\n",
        "wiki_page= []\n",
        "wiki_page2= []\n",
        "page_score= []\n",
        "diff= []\n",
        "for i in range(0, train.shape[0]):\n",
        "    ans_score= {}\n",
        "    for ii in train.loc[i]['IR_Wiki Scores'].split(', '):\n",
        "        ans_score[ii.split(':')[0]]= float(ii.split(':')[1])\n",
        "        \n",
        "    \n",
        "    page= sorted(ans_score, key= ans_score.get, reverse= True)[0]\n",
        "    page2= sorted(ans_score, key= ans_score.get, reverse= True)[1]\n",
        "    wiki_page.append(page)\n",
        "    page_score.append(ans_score[page])\n",
        "    wiki_page2.append(page2)\n",
        "    diff.append(ans_score[page]- ans_score[page2])\n",
        "\n",
        "train['Wiki_page']= wiki_page\n",
        "train['Wiki_page_embed']= wiki_page\n",
        "train['Wiki_page2']= wiki_page2\n",
        "train['Page score']= page_score\n",
        "train['Score difference']= diff\n",
        "print('Max IR score and corresponding page created for training data')\n",
        "\n",
        "\n",
        "\n",
        "# Max IR score and corresponding page (testing data)\n",
        "wiki_page= []\n",
        "wiki_page2= []\n",
        "page_score= []\n",
        "diff= []\n",
        "for i in range(0, test.shape[0]):\n",
        "    ans_score= {}\n",
        "    for ii in test.loc[i]['IR_Wiki Scores'].split(', '):\n",
        "        ans_score[ii.split(':')[0]]= float(ii.split(':')[1])\n",
        "        \n",
        "    \n",
        "    page= sorted(ans_score, key= ans_score.get, reverse= True)[0]\n",
        "    page2= sorted(ans_score, key= ans_score.get, reverse= True)[1]\n",
        "    wiki_page.append(page)\n",
        "    wiki_page2.append(page2)\n",
        "    page_score.append(ans_score[page])\n",
        "    diff.append(ans_score[page]- ans_score[page2])\n",
        "\n",
        "test['Wiki_page']= wiki_page\n",
        "test['Wiki_page_embed']= wiki_page\n",
        "test['Wiki_page2']= wiki_page2\n",
        "test['Page score']= page_score\n",
        "test['Score difference']= diff\n",
        "print('Max IR score and corresponding page created for testing data')\n",
        "\n",
        "\n",
        "\n",
        "################################################################\n",
        "\n",
        "# Target feature paren_match (training data). It is 1 if answer and wiki page match. 0 otherwise\n",
        "train['paren_match']= 0\n",
        "\n",
        "for i, row in train.iterrows():\n",
        "    if row['Answer'] == row['Wiki_page']:\n",
        "        train.loc[i, 'paren_match']= 1\n",
        "print('paren_match created for training data')\n",
        "\n",
        "\n",
        "\n",
        "#################################################################\n",
        "\n",
        "# Create a new column to embed categories (training data)\n",
        "train['category_embed']= train['category']\n",
        "\n",
        "\"\"\"\n",
        "encoding= ce.WOEEncoder(cols= ['category_embed'], impute_missing= True)\n",
        "encoding.fit(train[['category_embed']], train['paren_match'])\n",
        "train['category_embed']=encoding.transform(train[['category_embed']])\n",
        "print('Category embeddings created for training data')\n",
        "\n",
        "\n",
        "\n",
        "# Create a new column to embed categories (testing data)\n",
        "test['category_embed']= test['category']\n",
        "test['category_embed']= encoding.transform(test[['category_embed']])\n",
        "print('Category embeddings created for testing data')\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#################################################################\n",
        "\n",
        "\n",
        "# Create new topic columns for topic modeling (training data)\n",
        "train['Topic 0']= 0\n",
        "train['Topic 1']= 0\n",
        "train['Topic 2']= 0\n",
        "train['Topic 3']= 0\n",
        "print('Topics created for training data')\n",
        "\n",
        "\n",
        "# Create new topic columns for topic modeling (testing data)\n",
        "test['Topic 0']= 0\n",
        "test['Topic 1']= 0\n",
        "test['Topic 2']= 0\n",
        "test['Topic 3']= 0\n",
        "print('Topics created for testing data')\n",
        "\n",
        "\n",
        "##################################################################\n",
        "##################################################################\n",
        "\n",
        "\"\"\"\n",
        "Add new feature: topics (topic modeling)\n",
        "We have 4 categories in the dataset. SO lets use 4 topics\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Preprocessing the text first\n",
        "clean_questions= [clean(row['Question Text']).split() for _, row in train.iterrows()]\n",
        "print('Text preprocessing done')\n",
        "\n",
        "# Create a word to index mapping\n",
        "dictionary= corpora.Dictionary(clean_questions)\n",
        "print('Word index dictionary created')\n",
        "\n",
        "# Convert the list of documents to a BOW matrix using the dictionary prepared above\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_questions]\n",
        "print('BOW matrix created')\n",
        "\n",
        "# Train the LDA model on the document word matrix\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=4, id2word = dictionary, passes=50)\n",
        "print('LDA model trained')\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Add topic distributions as features to train and test data\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Topic distributions (training data)\n",
        "for i, row in train.iterrows():\n",
        "    for pair in ldamodel[doc_term_matrix[i]]:\n",
        "        col= 'Topic '+ str(pair[0])\n",
        "        train.loc[i, col]= pair[1]\n",
        "        \n",
        "print('Topic distributions added to training data')    \n",
        "    \n",
        "\n",
        "# Topic distributions (testing data)\n",
        "\n",
        "clean_test= [clean(row['Question Text']).split() for _, row in test.iterrows()]\n",
        "test_bow= [dictionary.doc2bow(doc) for doc in clean_test]\n",
        "\n",
        "for i, row in test.iterrows():\n",
        "    for pair in ldamodel.get_document_topics(test_bow[i]):\n",
        "        col= 'Topic '+ str(pair[0])\n",
        "        test.loc[i, col]= pair[1]\n",
        "\n",
        "print('Topic distributions added to testing data')\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question length created for training data\n",
            "Question length created for testing data\n",
            "Max IR score and corresponding page created for training data\n",
            "Max IR score and corresponding page created for testing data\n",
            "paren_match created for training data\n",
            "Topics created for training data\n",
            "Topics created for testing data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Topic distributions (training data)\\nfor i, row in train.iterrows():\\n    for pair in ldamodel[doc_term_matrix[i]]:\\n        col= 'Topic '+ str(pair[0])\\n        train.loc[i, col]= pair[1]\\n        \\nprint('Topic distributions added to training data')    \\n    \\n\\n# Topic distributions (testing data)\\n\\nclean_test= [clean(row['Question Text']).split() for _, row in test.iterrows()]\\ntest_bow= [dictionary.doc2bow(doc) for doc in clean_test]\\n\\nfor i, row in test.iterrows():\\n    for pair in ldamodel.get_document_topics(test_bow[i]):\\n        col= 'Topic '+ str(pair[0])\\n        test.loc[i, col]= pair[1]\\n\\nprint('Topic distributions added to testing data')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6njPXEQB5Mo",
        "colab_type": "text"
      },
      "source": [
        "## Split into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8KNoPGCJB5Mp",
        "colab_type": "code",
        "outputId": "3607104d-d98e-4e5a-fce4-cfa7768af8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train, train['paren_match'], test_size= 0.2, random_state= 4)\n",
        "print('X train shape: {}'.format(X_train.shape))\n",
        "print('y train shape: {}'.format(y_train.shape))\n",
        "print('X test shape: {}'.format(X_test.shape))\n",
        "print('y test shape: {}'.format(y_test.shape))\n",
        "\n",
        "X_train= X_train.reset_index(drop= True)\n",
        "y_train= y_train.reset_index(drop= True)\n",
        "X_test= X_test.reset_index(drop= True)\n",
        "y_test= y_test.reset_index(drop= True)\n",
        "\n",
        "print('reindexed X train and y train for WOE embeddings')\n",
        "print('reindexed X test and y test for WOE embeddings')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X train shape: (6528, 19)\n",
            "y train shape: (6528,)\n",
            "X test shape: (1632, 19)\n",
            "y test shape: (1632,)\n",
            "reindexed X train and y train for WOE embeddings\n",
            "reindexed X test and y test for WOE embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFg8YPQNB5Ms",
        "colab_type": "code",
        "outputId": "e3a1b4a1-b154-4cbe-c25a-030c6b0adec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoding= ce.WOEEncoder(cols= ['category_embed'])\n",
        "encoding.fit(X_train[['category_embed']], X_train['paren_match'])\n",
        "X_train['category_embed']=encoding.transform(X_train[['category_embed']])\n",
        "print('Category embeddings created for training data')\n",
        "\n",
        "# Create a new column to embed categories (testing data)\n",
        "X_test['category_embed']= encoding.transform(X_test[['category_embed']])\n",
        "print('Category embeddings created for testing data')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Category embeddings created for training data\n",
            "Category embeddings created for testing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JyWEA-oqB5Mw",
        "colab_type": "code",
        "outputId": "bfbd7684-f264-4a96-854e-2be660726496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\"\"\"\n",
        "Add new feature: topics (topic modeling)\n",
        "We have 4 categories in the dataset. SO lets use 4 topics\n",
        "\"\"\"\n",
        "# Preprocessing the text first\n",
        "clean_questions= [clean(row['Question Text']).split() for _, row in X_train.iterrows()]\n",
        "print('Text preprocessing done')\n",
        "\n",
        "# Create a word to index mapping\n",
        "dictionary= corpora.Dictionary(clean_questions)\n",
        "print('Word index dictionary created')\n",
        "\n",
        "# Convert the list of documents to a BOW matrix using the dictionary prepared above\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_questions]\n",
        "print('BOW matrix created')\n",
        "\n",
        "# Train the LDA model on the document word matrix\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=4, id2word = dictionary, passes=50)\n",
        "print('LDA model trained')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Text preprocessing done\n",
            "Word index dictionary created\n",
            "BOW matrix created\n",
            "LDA model trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyT3VU8DB5My",
        "colab_type": "code",
        "outputId": "150cf788-6384-4bf2-d6c8-e0549e574fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\"\"\"\n",
        "Add topic distributions as features to train and test data\n",
        "\"\"\"\n",
        "# Topic distributions (training data)\n",
        "for i, row in X_train.iterrows():\n",
        "    for pair in ldamodel[doc_term_matrix[i]]:\n",
        "        col= 'Topic '+ str(pair[0])\n",
        "        X_train.loc[i, col]= pair[1]\n",
        "        \n",
        "print('Topic distributions added to training data')    \n",
        "    \n",
        "\n",
        "# Topic distributions (testing data)\n",
        "\n",
        "clean_test= [clean(row['Question Text']).split() for _, row in X_test.iterrows()]\n",
        "new_bow= [dictionary.doc2bow(doc) for doc in clean_test]\n",
        "\n",
        "for i, row in X_test.iterrows():\n",
        "    for pair in ldamodel.get_document_topics(new_bow[i]):\n",
        "        col= 'Topic '+ str(pair[0])\n",
        "        X_test.loc[i, col]= pair[1]\n",
        "\n",
        "print('Topic distributions added to testing data')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Topic distributions added to training data\n",
            "Topic distributions added to testing data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4on1Hb9pM-It",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "31e45766-39aa-4c99-82ea-3ddf1561522e"
      },
      "source": [
        "print(X_train.head(15))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Question ID  ...   Topic 3\n",
            "0        153256  ...  0.000000\n",
            "1        109300  ...  0.000000\n",
            "2        115761  ...  0.348513\n",
            "3        184101  ...  0.011198\n",
            "4        176257  ...  0.180916\n",
            "5        101321  ...  0.000000\n",
            "6          4402  ...  0.000000\n",
            "7        118036  ...  0.028714\n",
            "8        168293  ...  0.773437\n",
            "9        105649  ...  0.000000\n",
            "10        10999  ...  0.230344\n",
            "11         7072  ...  0.000000\n",
            "12       154257  ...  0.566846\n",
            "13         4408  ...  0.012110\n",
            "14       182705  ...  0.471777\n",
            "\n",
            "[15 rows x 19 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnu21px4B5M1",
        "colab_type": "text"
      },
      "source": [
        "### features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zba-d6KdB5M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features= ['Quest len', 'Page score',  'category_embed', 'Score difference', 'Topic 0', 'Topic 1', 'Topic 2', 'Topic 3']\n",
        "target= ['paren_match']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTy6VhTXqXHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsDvGD__B5M4",
        "colab_type": "text"
      },
      "source": [
        "### regularize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Wd9WiGxAB5M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "feat_reg= ['Quest len', 'Page score',  'category_embed', 'Score difference']\n",
        "\n",
        "scaler= StandardScaler()\n",
        "\n",
        "scaler.fit(X_train[feat_reg].values)\n",
        "\n",
        "X_train[feat_reg]= scaler.transform(X_train[feat_reg].values)\n",
        "X_test[feat_reg]= scaler.transform(X_test[feat_reg].values)\n",
        "\n",
        "print('regularized X train and X test')\n",
        "\"\"\"\n",
        "\n",
        "y_train= np.reshape(y_train.values, (y_train.shape[0], ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ5-4ngeB5M7",
        "colab_type": "text"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfn0xoBpmhUz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yJ85wJctB5NL",
        "colab_type": "code",
        "outputId": "675b85b7-cf57-4eae-c311-4b2d826d8144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "source": [
        "svm_clf= SVC(kernel= 'rbf')\n",
        "\n",
        "corr, incorr= auto_ML(svm_clf, features, X_train, y_train, X_test, y_test, folds= 10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test prediction:\n",
            "[0 0 0 ... 1 1 1]\n",
            "-------------------------------------\n",
            "Accuracy score: 0.8045343137254902\n",
            "-------------------------------------\n",
            "confusion matrix:\n",
            " [[ 255  235]\n",
            " [  84 1058]]\n",
            "-------------------------------------\n",
            "Cross Validation score: 0.7827818171715302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6mP6qVRcB5NN",
        "colab_type": "code",
        "outputId": "2ac77db5-ed44-46cb-c68d-6a4f1ee7de70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "corr.describe()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Page score</th>\n",
              "      <th>Score difference</th>\n",
              "      <th>category_embed</th>\n",
              "      <th>Topic 0</th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1312.000000</td>\n",
              "      <td>1312.000000</td>\n",
              "      <td>1312.000000</td>\n",
              "      <td>1312.000000</td>\n",
              "      <td>1312.000000</td>\n",
              "      <td>1312.000000</td>\n",
              "      <td>1312.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>37.294122</td>\n",
              "      <td>11.180870</td>\n",
              "      <td>0.039555</td>\n",
              "      <td>0.316499</td>\n",
              "      <td>0.161893</td>\n",
              "      <td>0.297133</td>\n",
              "      <td>0.216502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>55.164444</td>\n",
              "      <td>20.035686</td>\n",
              "      <td>0.316887</td>\n",
              "      <td>0.348754</td>\n",
              "      <td>0.300512</td>\n",
              "      <td>0.364445</td>\n",
              "      <td>0.302805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.629610</td>\n",
              "      <td>1.509999</td>\n",
              "      <td>-0.308357</td>\n",
              "      <td>0.011156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.803140</td>\n",
              "      <td>3.987098</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>0.171268</td>\n",
              "      <td>0.014080</td>\n",
              "      <td>0.073319</td>\n",
              "      <td>0.031450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>45.515676</td>\n",
              "      <td>8.829453</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.595032</td>\n",
              "      <td>0.117665</td>\n",
              "      <td>0.569518</td>\n",
              "      <td>0.371850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>313.621279</td>\n",
              "      <td>175.442343</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.990343</td>\n",
              "      <td>0.988549</td>\n",
              "      <td>0.991099</td>\n",
              "      <td>0.991211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Page score  Score difference  ...      Topic 2      Topic 3\n",
              "count  1312.000000       1312.000000  ...  1312.000000  1312.000000\n",
              "mean     37.294122         11.180870  ...     0.297133     0.216502\n",
              "std      55.164444         20.035686  ...     0.364445     0.302805\n",
              "min       0.000000          0.000000  ...     0.000000     0.000000\n",
              "25%       5.629610          1.509999  ...     0.000000     0.000000\n",
              "50%       8.803140          3.987098  ...     0.073319     0.031450\n",
              "75%      45.515676          8.829453  ...     0.569518     0.371850\n",
              "max     313.621279        175.442343  ...     0.991099     0.991211\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KopCMksPB5NQ",
        "colab_type": "code",
        "outputId": "50241c10-8372-4be6-8d8d-65ea2df23e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "incorr.describe()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Page score</th>\n",
              "      <th>Score difference</th>\n",
              "      <th>category_embed</th>\n",
              "      <th>Topic 0</th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>40.244534</td>\n",
              "      <td>3.882680</td>\n",
              "      <td>-0.110158</td>\n",
              "      <td>0.276087</td>\n",
              "      <td>0.268614</td>\n",
              "      <td>0.247621</td>\n",
              "      <td>0.200916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>43.673396</td>\n",
              "      <td>6.592605</td>\n",
              "      <td>0.356306</td>\n",
              "      <td>0.351645</td>\n",
              "      <td>0.376792</td>\n",
              "      <td>0.344368</td>\n",
              "      <td>0.305104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.807185</td>\n",
              "      <td>0.485593</td>\n",
              "      <td>-0.308357</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>24.703295</td>\n",
              "      <td>1.273520</td>\n",
              "      <td>-0.308357</td>\n",
              "      <td>0.044640</td>\n",
              "      <td>0.026256</td>\n",
              "      <td>0.031940</td>\n",
              "      <td>0.020687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>64.112870</td>\n",
              "      <td>4.445684</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>0.538449</td>\n",
              "      <td>0.442999</td>\n",
              "      <td>0.393972</td>\n",
              "      <td>0.350941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>249.660259</td>\n",
              "      <td>55.106620</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.988216</td>\n",
              "      <td>0.988894</td>\n",
              "      <td>0.987527</td>\n",
              "      <td>0.988449</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Page score  Score difference  ...     Topic 2     Topic 3\n",
              "count  320.000000        320.000000  ...  320.000000  320.000000\n",
              "mean    40.244534          3.882680  ...    0.247621    0.200916\n",
              "std     43.673396          6.592605  ...    0.344368    0.305104\n",
              "min     -1.000000          0.000000  ...    0.000000    0.000000\n",
              "25%      4.807185          0.485593  ...    0.000000    0.000000\n",
              "50%     24.703295          1.273520  ...    0.031940    0.020687\n",
              "75%     64.112870          4.445684  ...    0.393972    0.350941\n",
              "max    249.660259         55.106620  ...    0.987527    0.988449\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VtkRR534B5NS",
        "colab_type": "code",
        "outputId": "e1aeb72a-f523-400e-ae8c-7488804b5464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "incorr[incorr['category']== 'social']['Question Text']"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1       the angelic liturgy is a minor text in this bo...\n",
              "21      as a baby this hero was left to die on a mount...\n",
              "45      one part of this work uses the metaphor of the...\n",
              "52      she turned a gardener into a frog after accept...\n",
              "64      according to one tradition it is the last of 1...\n",
              "80      he was the human grandfather of melicertes who...\n",
              "86      this deity owns hildesvini the battle boar whi...\n",
              "104     robert kapsis argued that it was low in certai...\n",
              "126     its name means house of stone although it is n...\n",
              "134     he was the human grandfather of melicertes who...\n",
              "169     cities on its shores include kibwesa wapembe a...\n",
              "185     in robert paul wolff 's reconstruction and cri...\n",
              "195     one son of this deity whacked travelers with a...\n",
              "200     a pope named for this figures greek name had a...\n",
              "222     eight of the ten books of the rig veda begin w...\n",
              "227     among this hero 's appearances in homer is as ...\n",
              "235     this figure was associated with the sycamore t...\n",
              "248     one work by this writer introduces the idea of...\n",
              "251     the kirk range and viphya mountains rise on th...\n",
              "256     some of his early works include the spirit of ...\n",
              "262     this being 's posessions include his altar sam...\n",
              "269     one of his works studies the influence of magi...\n",
              "271     in its second chapter the author tries to offe...\n",
              "280     this god is associated with the hieroglyph for...\n",
              "284     robert kapsis argued that it was low in certai...\n",
              "297     one work by this philosopher has a second sect...\n",
              "304     one son of this deity whacked travelers with a...\n",
              "313     some of the medieval sources about this god ma...\n",
              "322     he once won a fight with the god of death over...\n",
              "355     less famous members of this movement include p...\n",
              "                              ...                        \n",
              "1198    it compares the pursuit of pleasure by men to ...\n",
              "1206    pausanias claimed that this figure was raised ...\n",
              "1215    he seduced tyro the wife of his brother salmon...\n",
              "1251    under the guise of vegtam odin consulted with ...\n",
              "1258    while enslaved by queen omphale of lydia he wa...\n",
              "1268    the oldest known depiction of this god was fou...\n",
              "1271    a central tenet of this faith is the ik oknar ...\n",
              "1280    according to one tradition it is the last of 1...\n",
              "1311    one of this work 's key points is illustrated ...\n",
              "1373    in one story about him he is said to have sque...\n",
              "1379    this work describes a ritual in which the umbi...\n",
              "1397    god once helped out hera after she promised hi...\n",
              "1400    when this man was 13 he gave his parents an es...\n",
              "1402    he sent out the talking bird karshipta to spre...\n",
              "1406    this figure once fended off an attack by the c...\n",
              "1408    one of this thinker 's essays begins by quotin...\n",
              "1424    the deities in this religion get their orders ...\n",
              "1425    in the beginning of this book three jewish ser...\n",
              "1446    this man wrote about the epicurean the stoic a...\n",
              "1460    in the 1670s he wrote a verse autobiography in...\n",
              "1483    with libya this god is the father of belus age...\n",
              "1522    the deities in this religion get their orders ...\n",
              "1544    until the 1930s this woman wrote poetry under ...\n",
              "1551    richard cloward and lloyd ohlin attempted to m...\n",
              "1569    this figure once carried off a nymph named leu...\n",
              "1613    one of this work 's key points is illustrated ...\n",
              "1615    in the beginning of this book three jewish ser...\n",
              "1616    one work by this man attempted to disprove the...\n",
              "1622    this figure caused a group of men led by heini...\n",
              "1631    anecdotes about the deeds of this religious le...\n",
              "Name: Question Text, Length: 109, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXdZMwSuB5NU",
        "colab_type": "code",
        "outputId": "a9466a94-fdd4-44d5-d265-f6051011ef28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "X_test['category'].value_counts()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lit        611\n",
              "history    461\n",
              "social     360\n",
              "science    200\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJLKJBVzB5NV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d7e2513-49e0-4f78-ee16-45ae3759e497"
      },
      "source": [
        "X_train.head(15)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question ID</th>\n",
              "      <th>Question Text</th>\n",
              "      <th>QANTA Scores</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Sentence Position</th>\n",
              "      <th>IR_Wiki Scores</th>\n",
              "      <th>category</th>\n",
              "      <th>Quest len</th>\n",
              "      <th>Wiki_page</th>\n",
              "      <th>Wiki_page_embed</th>\n",
              "      <th>Wiki_page2</th>\n",
              "      <th>Page score</th>\n",
              "      <th>Score difference</th>\n",
              "      <th>paren_match</th>\n",
              "      <th>category_embed</th>\n",
              "      <th>Topic 0</th>\n",
              "      <th>Topic 1</th>\n",
              "      <th>Topic 2</th>\n",
              "      <th>Topic 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>153256</td>\n",
              "      <td>the name originally belonged to a chief of the...</td>\n",
              "      <td>aaron_burr:0.194713107831, tammany_hall:0.0927...</td>\n",
              "      <td>tammany_hall</td>\n",
              "      <td>1</td>\n",
              "      <td>aaron_burr:5.77478343774, alexander_hamilton:4...</td>\n",
              "      <td>history</td>\n",
              "      <td>282</td>\n",
              "      <td>aaron_burr</td>\n",
              "      <td>aaron_burr</td>\n",
              "      <td>alexander_hamilton</td>\n",
              "      <td>5.774783</td>\n",
              "      <td>1.650241</td>\n",
              "      <td>0</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.971204</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>109300</td>\n",
              "      <td>its leader was killed by mengli-geray after a ...</td>\n",
              "      <td>golden_horde:0.921987159189, kublai_khan:0.002...</td>\n",
              "      <td>golden_horde</td>\n",
              "      <td>2</td>\n",
              "      <td>golden_horde:10.7242201667, genghis_khan:2.749...</td>\n",
              "      <td>history</td>\n",
              "      <td>409</td>\n",
              "      <td>golden_horde</td>\n",
              "      <td>golden_horde</td>\n",
              "      <td>genghis_khan</td>\n",
              "      <td>10.724220</td>\n",
              "      <td>7.974561</td>\n",
              "      <td>1</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.981098</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>115761</td>\n",
              "      <td>one resident of this state served as secretary...</td>\n",
              "      <td>maine:0.0513319951667, william_mckinley:0.0512...</td>\n",
              "      <td>maine</td>\n",
              "      <td>3</td>\n",
              "      <td>hubert_humphrey:3.50538015338, george_wallace:...</td>\n",
              "      <td>history</td>\n",
              "      <td>454</td>\n",
              "      <td>hubert_humphrey</td>\n",
              "      <td>hubert_humphrey</td>\n",
              "      <td>george_wallace</td>\n",
              "      <td>3.505380</td>\n",
              "      <td>0.533030</td>\n",
              "      <td>0</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.982056</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>184101</td>\n",
              "      <td>the wu-yang solution to the yang-mills field e...</td>\n",
              "      <td>magnetic_monopole:0.806297691429, poynting_vec...</td>\n",
              "      <td>magnetic_monopole</td>\n",
              "      <td>1</td>\n",
              "      <td>quantum_hall_effect:36.477413, displacement_cu...</td>\n",
              "      <td>science</td>\n",
              "      <td>240</td>\n",
              "      <td>magnetic_monopole</td>\n",
              "      <td>magnetic_monopole</td>\n",
              "      <td>photon</td>\n",
              "      <td>68.566459</td>\n",
              "      <td>13.538780</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.011311</td>\n",
              "      <td>0.966536</td>\n",
              "      <td>0.011082</td>\n",
              "      <td>0.011071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>176257</td>\n",
              "      <td>wolfgang ketterle was awarded a nobel prize fo...</td>\n",
              "      <td>bose-einstein_condensate:0.265955065478, satye...</td>\n",
              "      <td>bose-einstein_condensate</td>\n",
              "      <td>1</td>\n",
              "      <td>james_clerk_maxwell:32.158349, albert_schweitz...</td>\n",
              "      <td>science</td>\n",
              "      <td>301</td>\n",
              "      <td>rubidium</td>\n",
              "      <td>rubidium</td>\n",
              "      <td>fermionic_condensate</td>\n",
              "      <td>68.990852</td>\n",
              "      <td>0.923727</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.176174</td>\n",
              "      <td>0.808232</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>101321</td>\n",
              "      <td>one derivative of this molecule is the mushroo...</td>\n",
              "      <td>hydrazine:0.262803870649, azide:0.038854836003...</td>\n",
              "      <td>hydrazine</td>\n",
              "      <td>3</td>\n",
              "      <td>glucose:66.824918, combustion:89.666787, aldeh...</td>\n",
              "      <td>science</td>\n",
              "      <td>695</td>\n",
              "      <td>hydrazine</td>\n",
              "      <td>hydrazine</td>\n",
              "      <td>bleach</td>\n",
              "      <td>190.988280</td>\n",
              "      <td>86.687014</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.987076</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4402</td>\n",
              "      <td>one important figure in this conflict later le...</td>\n",
              "      <td>taiping_rebellion:0.0565281519044, tang_dynast...</td>\n",
              "      <td>taiping_rebellion</td>\n",
              "      <td>3</td>\n",
              "      <td>taiping_rebellion:4.4651097851, boxer_rebellio...</td>\n",
              "      <td>history</td>\n",
              "      <td>319</td>\n",
              "      <td>taiping_rebellion</td>\n",
              "      <td>taiping_rebellion</td>\n",
              "      <td>boxer_rebellion</td>\n",
              "      <td>4.465110</td>\n",
              "      <td>1.228577</td>\n",
              "      <td>1</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.977494</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>118036</td>\n",
              "      <td>ostwald ripening is a consequence of this quan...</td>\n",
              "      <td>ammonia:0.258013455934, heat_capacity:0.069410...</td>\n",
              "      <td>gibbs_free_energy</td>\n",
              "      <td>0</td>\n",
              "      <td>magnetic_field:13.457197, rayleigh_scattering:...</td>\n",
              "      <td>science</td>\n",
              "      <td>82</td>\n",
              "      <td>momentum</td>\n",
              "      <td>momentum</td>\n",
              "      <td>quantum_dot</td>\n",
              "      <td>21.709716</td>\n",
              "      <td>2.180019</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.028448</td>\n",
              "      <td>0.914861</td>\n",
              "      <td>0.028157</td>\n",
              "      <td>0.028534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>168293</td>\n",
              "      <td>the critic edward garnett encouraged this man ...</td>\n",
              "      <td>joseph_conrad:0.165919060316, arthur_miller:0....</td>\n",
              "      <td>joseph_conrad</td>\n",
              "      <td>2</td>\n",
              "      <td>joseph_conrad:9.04399973674, d._h._lawrence:2....</td>\n",
              "      <td>lit</td>\n",
              "      <td>372</td>\n",
              "      <td>joseph_conrad</td>\n",
              "      <td>joseph_conrad</td>\n",
              "      <td>d._h._lawrence</td>\n",
              "      <td>9.044000</td>\n",
              "      <td>6.227725</td>\n",
              "      <td>1</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>0.978911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>105649</td>\n",
              "      <td>one version of it named for entner and doudoro...</td>\n",
              "      <td>glycolysis:0.164142172849, rubisco:0.083784352...</td>\n",
              "      <td>glycolysis</td>\n",
              "      <td>1</td>\n",
              "      <td>beta_oxidation:45.796817, dna_replication:50.2...</td>\n",
              "      <td>science</td>\n",
              "      <td>330</td>\n",
              "      <td>glycolysis</td>\n",
              "      <td>glycolysis</td>\n",
              "      <td>hexokinase</td>\n",
              "      <td>104.950963</td>\n",
              "      <td>47.707552</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.607354</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.974720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10999</td>\n",
              "      <td>the relationship this graph predicted was actu...</td>\n",
              "      <td>phillips_curve:0.156369707317, john_maynard_ke...</td>\n",
              "      <td>phillips_curve</td>\n",
              "      <td>0</td>\n",
              "      <td>piltdown_man:21.061110, arthur_conan_doyle:21....</td>\n",
              "      <td>social</td>\n",
              "      <td>155</td>\n",
              "      <td>fullerene</td>\n",
              "      <td>fullerene</td>\n",
              "      <td>laffer_curve</td>\n",
              "      <td>32.842373</td>\n",
              "      <td>5.578007</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.308357</td>\n",
              "      <td>0.438055</td>\n",
              "      <td>0.522124</td>\n",
              "      <td>0.019771</td>\n",
              "      <td>0.020050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>7072</td>\n",
              "      <td>the husband 's affair with barbara lynch and h...</td>\n",
              "      <td>the_sorrows_of_young_werther:0.0767418268067, ...</td>\n",
              "      <td>love_in_the_time_of_cholera</td>\n",
              "      <td>2</td>\n",
              "      <td>love_in_the_time_of_cholera:5.24855733538, jun...</td>\n",
              "      <td>lit</td>\n",
              "      <td>556</td>\n",
              "      <td>love_in_the_time_of_cholera</td>\n",
              "      <td>love_in_the_time_of_cholera</td>\n",
              "      <td>jun'ichiro_tanizaki</td>\n",
              "      <td>5.248557</td>\n",
              "      <td>2.818967</td>\n",
              "      <td>1</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>0.141346</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.848763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>154257</td>\n",
              "      <td>she is teaching at a charity school in morton ...</td>\n",
              "      <td>jane_eyre:0.374268008993, jane_eyre_(character...</td>\n",
              "      <td>jane_eyre</td>\n",
              "      <td>1</td>\n",
              "      <td>jane_eyre:11.0988569471, pride_and_prejudice:3...</td>\n",
              "      <td>lit</td>\n",
              "      <td>239</td>\n",
              "      <td>jane_eyre</td>\n",
              "      <td>jane_eyre</td>\n",
              "      <td>pride_and_prejudice</td>\n",
              "      <td>11.098857</td>\n",
              "      <td>7.676591</td>\n",
              "      <td>1</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010223</td>\n",
              "      <td>0.970130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4408</td>\n",
              "      <td>one ruler of this dynasty avoided a prophecy o...</td>\n",
              "      <td>safavid_dynasty:0.126599919267, chandragupta_m...</td>\n",
              "      <td>safavid_dynasty</td>\n",
              "      <td>1</td>\n",
              "      <td>safavid_dynasty:5.68458551105, tang_dynasty:3....</td>\n",
              "      <td>history</td>\n",
              "      <td>219</td>\n",
              "      <td>safavid_dynasty</td>\n",
              "      <td>safavid_dynasty</td>\n",
              "      <td>tang_dynasty</td>\n",
              "      <td>5.684586</td>\n",
              "      <td>2.066927</td>\n",
              "      <td>1</td>\n",
              "      <td>0.355110</td>\n",
              "      <td>0.012274</td>\n",
              "      <td>0.012043</td>\n",
              "      <td>0.963578</td>\n",
              "      <td>0.012105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>182705</td>\n",
              "      <td>this author wrote a series of letters that wer...</td>\n",
              "      <td>oliver_goldsmith:0.877327329682, franz_kafka:0...</td>\n",
              "      <td>oliver_goldsmith</td>\n",
              "      <td>0</td>\n",
              "      <td>oliver_goldsmith:9.84777532293, charles_dicken...</td>\n",
              "      <td>lit</td>\n",
              "      <td>202</td>\n",
              "      <td>oliver_goldsmith</td>\n",
              "      <td>oliver_goldsmith</td>\n",
              "      <td>charles_dickens</td>\n",
              "      <td>9.847775</td>\n",
              "      <td>7.151611</td>\n",
              "      <td>1</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>0.961703</td>\n",
              "      <td>0.012590</td>\n",
              "      <td>0.012754</td>\n",
              "      <td>0.012953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Question ID  ...   Topic 3\n",
              "0        153256  ...  0.000000\n",
              "1        109300  ...  0.000000\n",
              "2        115761  ...  0.000000\n",
              "3        184101  ...  0.011071\n",
              "4        176257  ...  0.000000\n",
              "5        101321  ...  0.000000\n",
              "6          4402  ...  0.000000\n",
              "7        118036  ...  0.028534\n",
              "8        168293  ...  0.000000\n",
              "9        105649  ...  0.000000\n",
              "10        10999  ...  0.020050\n",
              "11         7072  ...  0.848763\n",
              "12       154257  ...  0.970130\n",
              "13         4408  ...  0.012105\n",
              "14       182705  ...  0.012953\n",
              "\n",
              "[15 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkugjKIgB5NX",
        "colab_type": "text"
      },
      "source": [
        "### Feedforward net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh2u2oQrB5NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Prepare input\n",
        "\"\"\"\n",
        "Xffn= np.array(X_train[features])\n",
        "yffn= np.array(y_train)\n",
        "\n",
        "Xtestffn= np.array(X_test[features])\n",
        "ytestffn= np.array(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3xdeLNKss9y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eFURUcDaB5NZ",
        "colab_type": "code",
        "outputId": "f48d2a8a-66ce-4e5c-8962-6f793716070d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "tokenizer = Tokenizer(nb_words=2000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                                   lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(X_train['Question Text'].values)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vqXQ-6Nfd5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "309bcea4-24f0-41cb-c428-bfbdd883ccd9"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f7cbcbc3240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zImgpFq5fiOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain = tokenizer.texts_to_sequences(X_train['Question Text'].values)\n",
        "Xtest = tokenizer.texts_to_sequences(X_test['Question Text'].values)\n",
        "\n",
        "X = pad_sequences(Xtrain,maxlen = 100,truncating = 'post')\n",
        "Y = pad_sequences(Xtest, maxlen = 100, truncating = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8ZZ0VFjfrgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "6ac52e34-aa1c-430b-d159-75dbf6b3edb5"
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "batch_size = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(2000, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
        "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
        "model.add(Dense(1,activation='softmax'))\n",
        "model.compile(loss = 'mean_squared_error', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0818 03:47:00.093438 140176621692800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
            "  \n",
            "W0818 03:47:00.134445 140176621692800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0818 03:47:00.141868 140176621692800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  import sys\n",
            "W0818 03:47:00.328561 140176621692800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0818 03:47:00.341454 140176621692800 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0818 03:47:00.604862 140176621692800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 128)          256000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 197       \n",
            "=================================================================\n",
            "Total params: 510,997\n",
            "Trainable params: 510,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOrua8mPg7QN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "8329918f-6ac5-43e8-8c18-5ac5c4ed0e39"
      },
      "source": [
        "\n",
        "model.fit(X, y_train, batch_size = batch_size, nb_epoch = 3, verbose = 2)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n",
            "W0818 03:47:05.495073 140176621692800 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0818 03:47:06.114328 140176621692800 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " - 42s - loss: 0.3197 - acc: 0.6803\n",
            "Epoch 2/3\n",
            " - 33s - loss: 0.3197 - acc: 0.6803\n",
            "Epoch 3/3\n",
            " - 33s - loss: 0.3197 - acc: 0.6803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7cbc26deb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKv6n424i-Hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be9c5590-3b30-4fca-b2cd-dadc67110c03"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.sequential.Sequential object at 0x7f7cbc280d30>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-v6O79pjBfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "900c0db0-bbf1-468b-9779-497509750acf"
      },
      "source": [
        "score,acc = model.evaluate(Y, y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"Score: %.2f\" % (score))\n",
        "print(\"Validation Accuracy: %.2f\" % (acc))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.30\n",
            "Validation Accuracy: 0.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VofV2G-4m-pB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14dccb7a-29f6-4a62-99da-21e34c342af7"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6528, 147)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqj3j6FRzKJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}